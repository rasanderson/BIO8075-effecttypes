---
title: "Manual calculation of effect sizes and meta-analysis"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(metafor)
knitr::opts_chunk$set(echo = FALSE)
```


## Introduction
This website introduces you to two types of meta-analysis, **fixed-effects** and **random-effects** analysis, and also shows you how to calculate effect sizes manually, and hence the mechanics of a simple meta-analysis. In most of this module you will use automatic methods to calculate effect sizes and do the meta-analysis, but doing some of the basics manually will give you a better understanding. Finally, we address the problem that when you are collating information from several different studies they may not all report the information in the same way. For example one paper may have the means, standard errors and sample sizes, making it easy to calculate the standardised mean differences. However, another might report means, t-statistics and p-values. These cannot all be analysed automatically until they are converted to the same format.

To do this you need to understand more about the mechanics of calculating effect sizes, which (sorry!) results in a few equations to work through in this website.

### Vectorised arithmetic
Manual calculation of effect sizes is made easier by R's **vectorised arithmetic** which means that you can apply the same  calculation to every element in a table of data. For example, the following code block contains a `data.frame` called `study_dat` with two columns `x1` and `v1`. If you wish to calculate a third column that contains `x1` multiplied by `v1` this can be done simultaneously down every element of the of the column. You can also access e.g. the third row using `study_dat[3,]`, or the second column with `study_dat[,2]` or `study_dat$v1`. To access the fourth entry in the second column use `study_dat[4,2]` or `study_dat$v1[4]`. You might to access individual entries in a table of data when converting from one effect size type to another if reporting is inconsistent between studies.

Play around with the `study_dat` data in the following code box to familiarise yourself with how to access individual rows or columns or entries in a table of data. What do you think `NA` represents, and what happens when it occurs in calculations?

```{r vectorised-setup}
study_dat <- data.frame(x1 = c(4, 5, 7, 9, 0, 2), v1 = c( 3.2, 1.7, 9.8, NA, 4.0, 3.3))
```

```{r vectorised, exercise=TRUE}
# Display study_dat
study_dat
# Create a new column by multiplication
study_dat$x1v1 <- study_dat$x1 * study_dat$v1
study_dat
# Display 3rd row
study_dat[3,]
# Display fourth entry of x1
study_dat$x1[4]
```

## Fixed- vs Random-effects
### Reminder of simple meta-analysis and forest plot
First, a quick revision of some basic concepts. Meta-analysis is basically a weighted average of standardised effect sizes. We need to use standardised effect sizes as different studies may have used different measures, units etc. and so standardising by the pooled standard deviation makes them easier to compare. The weights are derived from the sample size and variation in each study. Forest plots provide a simple summary of a meta-analysis; here is the example from [the earlier website](https://naturalandenvironmentalscience.shinyapps.io/Effectsizes/#section-interactive-forest-plot-of-meta-analysis), taken from Borenstein:

![](www/forest_fig_1.png){width=75%}

Here the squares represent the standardised effect size for the individual studies and the size of the squares their weights. The diamond represents the overall weighted average of the effect size, and the width of the diamond represents the degree of uncertaintity about the overall estimate.

### Fixed-effects meta-analysis
In a fixed effect analysis, all the studies are assumed to have the same ‘true’ effect, and all the factors that could influence the effect size are the same in all the studies. However, there is still sampling error, which is reflected in the different results for each study The study weights are assigned as the inverse of the variance of each study. This can be summed up in the following diagram:

![](www/fixed_fig_11_1_11_2.png){width=75%}

So the key point with a fixed-effect model is that the observed effect $Y_i$ for any given study $i$ is:

$$Y_i = \theta + \epsilon_i$$
where $\theta$ (Greek letter _theta_) is the overall "population" mean and $\epsilon_i$ (Greek letter _epsilon_) is the sampling error (which might be positive or negative). Have a look at [this  website](https://naturalandenvironmentalscience.shinyapps.io/Effectsizes/#section-populations-and-samples) to remind yourself of populations and samples.

### Random effects models
One problem with the fixed-effect method is that it assumes that all the studies are identical, and that the real effect size is the same in all the studies. Of course in reality this is unlikely to be true. The underlying effect sizes are likely to differ.  For example, survival of an endangered mammal population as a result of a management intervention may partly depend on the age of the mammals in the different populations in each of the different studies, as well as the actual management intervention. The diagram below shows the true effect sizes (0.55, 0.65 and 0.50) in three studies, assuming no observation error whatsoever, $\delta_1$, $\delta_2$ and $\delta_3$. The curve represents the normal distribution around all true effects.

![](www/rand_effect_fig_12_2.png){width=75%}

Of course in reality, there is observation error, so for our third study there is both the sampling error, shown as $\epsilon_3$ below which is the difference of what was observed, $Y_3$ and the true effect size $\delta_3$ in that study. The study effect size differs from the overall effect size for all studies by $\zeta_3$ as shown below:

![](www/rand_effect_fig_12_3.png){width=75%}

So, we can say that the observed effect $Y_i$ for any study is given by the grand mean $\mu$ (Greek letter _mu_), the deviation of the study's true effect from the grand mean $\zeta_i$ (Greek letter _zeta_), and the sampling error in that study $\epsilon_i$ (Greek letter _epsilon_):

$$Y_i = \mu + \zeta_i + \epsilon_i$$
This equation covers both within-study variation $\epsilon_i$ and between-study variation $\zeta_i$.


