---
title: "Manual calculation of effect sizes and meta-analysis"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(metafor)
continuous_dat <- read.csv("www/Borenstein_p88.csv")
binary_dat     <- read.csv("www/Borenstein_p93.csv")
corr_dat       <- read.csv("www/Borenstein_p98.csv")
knitr::opts_chunk$set(echo = FALSE)
```


## Introduction
This website introduces you to two types of meta-analysis, **fixed-effects** and **random-effects** analysis, and also shows you how to calculate effect sizes manually, and hence the mechanics of a simple meta-analysis. In most of this module you will use automatic methods to calculate effect sizes and do the meta-analysis, but doing some of the basics manually will give you a better understanding. Finally, we address the problem that when you are collating information from several different studies they may not all report the information in the same way. For example one paper may have the means, standard errors and sample sizes, making it easy to calculate the standardised mean differences. However, another might report means, t-statistics and p-values. These cannot all be analysed automatically until they are converted to the same format.

To do this you need to understand more about the mechanics of calculating effect sizes, which (sorry!) results in a few equations to work through in this website.

### Vectorised arithmetic
Manual calculation of effect sizes is made easier by R's **vectorised arithmetic** which means that you can apply the same  calculation to every element in a table of data. For example, the following code block contains a `data.frame` called `study_dat` with two columns `x1` and `v1`. If you wish to calculate a third column that contains `x1` multiplied by `v1` this can be done simultaneously down every element of the of the column. You can also access e.g. the third row using `study_dat[3,]`, or the second column with `study_dat[,2]` or `study_dat$v1`. To access the fourth entry in the second column use `study_dat[4,2]` or `study_dat$v1[4]`. You might to access individual entries in a table of data when converting from one effect size type to another if reporting is inconsistent between studies.

Play around with the `study_dat` data in the following code box to familiarise yourself with how to access individual rows or columns or entries in a table of data. What do you think `NA` represents, and what happens when it occurs in calculations?

```{r vectorised-setup}
study_dat <- data.frame(x1 = c(4, 5, 7, 9, 0, 2), v1 = c( 3.2, 1.7, 9.8, NA, 4.0, 3.3))
```

```{r vectorised, exercise=TRUE}
# Display study_dat
study_dat
# Create a new column by multiplication
study_dat$x1v1 <- study_dat$x1 * study_dat$v1
study_dat
# Display 3rd row
study_dat[3,]
# Display fourth entry of x1
study_dat$x1[4]
```

## Fixed- vs Random-effects
### Reminder of simple meta-analysis and forest plot
First, a quick revision of some basic concepts. Meta-analysis is basically a weighted average of standardised effect sizes. We need to use standardised effect sizes as different studies may have used different measures, units etc. and so standardising by the pooled standard deviation makes them easier to compare. The weights are derived from the sample size and variation in each study. Forest plots provide a simple summary of a meta-analysis; here is the example from [the earlier website](https://naturalandenvironmentalscience.shinyapps.io/Effectsizes/#section-interactive-forest-plot-of-meta-analysis), taken from Borenstein:

![](www/forest_fig_1.png){width=75%}

Here the squares represent the standardised effect size for the individual studies and the size of the squares their weights. The diamond represents the overall weighted average of the effect size, and the width of the diamond represents the degree of uncertaintity about the overall estimate.

### Fixed-effects meta-analysis
In a fixed effect analysis, all the studies are assumed to have the same ‘true’ effect, and all the factors that could influence the effect size are the same in all the studies. However, there is still sampling error, which is reflected in the different results for each study The study weights are assigned as the inverse of the variance of each study. This can be summed up in the following diagram:

![](www/fixed_fig_11_1_11_2.png){width=75%}

So the key point with a fixed-effect model is that the observed effect $Y_i$ for any given study $i$ is:

$$Y_i = \theta + \epsilon_i$$
where $\theta$ (Greek letter _theta_) is the overall "population" mean and $\epsilon_i$ (Greek letter _epsilon_) is the sampling error (which might be positive or negative). Have a look at [this  website](https://naturalandenvironmentalscience.shinyapps.io/Effectsizes/#section-populations-and-samples) to remind yourself of populations and samples.

### Random effects models
One problem with the fixed-effect method is that it assumes that all the studies are identical, and that the real effect size is the same in all the studies. Of course in reality this is unlikely to be true. The underlying effect sizes are likely to differ.  For example, survival of an endangered mammal population as a result of a management intervention may partly depend on the age of the mammals in the different populations in each of the different studies, as well as the actual management intervention. The diagram below shows the true effect sizes (0.55, 0.65 and 0.50) in three studies, assuming no observation error whatsoever, $\delta_1$, $\delta_2$ and $\delta_3$. The curve represents the normal distribution around all true effects.

![](www/rand_effect_fig_12_2.png){width=75%}

Of course in reality, there is observation error, so for our third study there is both the sampling error, shown as $\epsilon_3$ below which is the difference of what was observed, $Y_3$ and the true effect size $\delta_3$ in that study. The study effect size differs from the overall effect size for all studies by $\zeta_3$ as shown below:

![](www/rand_effect_fig_12_3.png){width=75%}

So, we can say that the observed effect $Y_i$ for any study is given by the grand mean $\mu$ (Greek letter _mu_), the deviation of the study's true effect from the grand mean $\zeta_i$ (Greek letter _zeta_), and the sampling error in that study $\epsilon_i$ (Greek letter _epsilon_):

$$Y_i = \mu + \zeta_i + \epsilon_i$$
This equation covers both within-study variation $\epsilon_i$ and between-study variation $\zeta_i$.

## Effect sizes for continuous data
For this example we will use a set of data in Borenstein page 88, in which there are six studies, for which we know the **means, standard deviations and sample sizes** of two interventions, treatment `T` and the control `C`. These are three commonly used effect sizes for continuous data:

* Standardised mean difference _d_
* Hedge's _g_ (bias corrected)
* Response ratios (log-transformed)

You might be wondering why we can't simply use the raw (unstandardised) difference between the means. There might be some exceptions where this can be used, for example all the studies have similar variances, and all the studies use precisely the same scale of measurement etc. In practice, these prerequisites are rarely met with ecological data.

The relevant data are contained in the R `data.frame` called `continuous_dat`:

```{r, echo=FALSE}
print(continuous_dat)
```


### Standardised mean difference _d_
First we want to calculate the standardised mean difference _d_ for
treatment and control in each study, which is done via:

$$d=\frac{\overline{X_1}-\overline{X_2}}{S_{within}}$$

where $\overline{X_1}$ and $\overline{X_2}$ are the means of the Treatment and Control of the study, and ${S_{within}}$ is the within-groups standard deviation. The latter can be calculated from:

$$S_{within}=\sqrt{\frac{(n_1-1)S^2_1 + (n2-1)S^2_2}  {n_1 + n_2 - 2}}$$

where $n_1$ and $n_2$ are the numbers of replicates for the two treatments in the study, whilst $S^2_1$ and $S^2_1$ are the respective standard deviations. We can easily calculate these two values for the first study, by Carroll, in R:

```{r manual_calc_Carroll, exercise=TRUE}
S_within_Carroll <- sqrt(((60-1)*22^2 +(60-1)*20^2) / (60 + 60 -2))
d_Carroll        <- (94 - 92) / S_within_Carroll
S_within_Carroll
d_Carroll


```

It would take a long time too long to do each calculation for every study, so this is where the vectorised arithmetic is useful, so you can find the results for all the studies simultaneously:

```{r manual_calc_S_within_d, exercise=TRUE}
S_within <- sqrt(((continuous_dat$T_n-1)*continuous_dat$T_SD^2 + (continuous_dat$C_n-1)*continuous_dat$C_SD^2) / 
                   (continuous_dat$T_n + continuous_dat$C_n -2))
d       <- (continuous_dat$T_mean - continuous_dat$C_mean) / S_within
cbind(S_within, d)

```

In above table `S_within` is the within-groups standard deviation, and _d_ is the standardised mean difference. We now want to calculate the variance of this standardised mean difference, $V_d$ :

$$V_d = \frac{n_1 + n_2}{n_1n_2}+\frac{d^2}{2(n_1+n_2)}$$

In the above equation, the first term on the right, $\frac{n_1 + n_2}{n_1n_2}$, represents the uncertainty in the difference betweens the means, $\overline{X_1}-\overline{X_2}$ . The second term on the right, $\frac{d^2}{2(n_1+n_2)}$, reflects the uncertainty of our estimate in the within-groups standard deviation $S_{within}$, calculated earlier.

We can calculate $V_d$ for the Carroll study first:

```{r Vd_Carroll-setup}
S_within_Carroll <- sqrt(((60-1)*22^2 +(60-1)*20^2) / (60 + 60 -2))
d_Carroll        <- (94 - 92) / S_within_Carroll

```

```{r Vd_Carroll, exercise=TRUE}
Vd_Carroll <- (60+60)/(60*60) + d_Carroll^2/(2*(60+60))
Vd_Carroll

```
Again, you have to be careful with brackets to ensure that the additions and multiplications etc. are down in the correct order.

`d_Carroll` represents _d_ or the standardised mean difference, also known as _Cohen's d_ .  Unfortunately _d_ has a slight bias in small sample sizes, and so needs to be fixed with a correction factor _J_ to calculate _g_ or _Hedge's g_, which is a more robust measure of the standardised mean difference. Most meta-analyses use _Hedge's g_ automatically, and the correction is easy to calculate:

$$J = 1 - \frac{3}{4df -1}$$
where $df$ is the degrees of freedom used to estimate $S_{within}$, which for two independent groups is simply $n_1+n_2-2$ (see equation fo $S_{within})$).

### Hedge's _g_ 
The standardised mean difference _d_ is less reliable for studies of small sample size. For this reason, _Hedge's g_ is often a preferred effect size data, as it contains a "bias correction" term _J_:

$$g = J . d$$
Whilst the bias-correct variance $V_g$ is:
$$V_g = J^2 . V_d$$

We can now readily do these calculations in R for the Carroll study:

```{r J_Hedges_g_Carroll-setup}
S_within_Carroll <- sqrt(((60-1)*22^2 +(60-1)*20^2) / (60 + 60 -2))
d_Carroll        <- (94 - 92) / S_within_Carroll
Vd_Carroll <- (60+60)/(60*60) + d_Carroll^2/(2*(60+60))

```

```{r J_Hedges_g_Carroll, exercise=TRUE}
 # There were 60 replicates in each T and C
J_Carroll <- 1 - (3/(4 * 118 - 1))
g_Carroll <- J_Carroll * d_Carroll
Vg_Carroll <- J_Carroll^2 * Vd_Carroll
J_Carroll
g_Carroll
Vg_Carroll
```

Obviously, it would take a long time to calculate each of these values for all the studies separately, but again we can take advantage of R's vectorised system to calculate _J_ and _Hedge's g_ for all the studies in one go:

```{r J_Hedges_g_Vg_all-setup}
S_within <- sqrt(((continuous_dat$T_n-1)*continuous_dat$T_SD^2 +
                   (continuous_dat$C_n-1)*continuous_dat$C_SD^2) / 
                   (continuous_dat$T_n + continuous_dat$C_n -2))
d       <- (continuous_dat$T_mean - continuous_dat$C_mean) / S_within

```


```{r J_Hedges_g_Vg_all, exercise=TRUE}
J <- 1 - (3 / (4 * (continuous_dat$T_n + continuous_dat$C_n - 2) -1))
g <- J * d
Vd <- (continuous_dat$T_n+continuous_dat$C_n)/(continuous_dat$T_n*continuous_dat$C_n) +
   d^2/(2*(continuous_dat$T_n+continuous_dat$C_n))
Vg <- J^2 * Vd
cbind(J, d, g, Vd, Vg)

```

You can see from the above that the bias-correction values, _J_ are all near 1.0, but it is lower for the 3rd study, which has the smallest number of samples.  Values of J near to 1.0 indicate that less bias-correction is needed.

### Response ratios
Where the outcome of the experiment or study is on a real physical scale, such as animal weight, tree height etc. with a true zero (no negative numbers possible) then the ratio of the two means (e.g. intervention and control) provides a useful and relatively straightforward effect size measurement. For the reasons outlined [here](https://naturalandenvironmentalscience.shinyapps.io/Effectsizes/#section-how-to-measure-effect-sizes) these are usually log-transformed, so that they are symmetrical and more likely to be normally distributed.

Thus, the log response ratio is:

$$ln(RR)=\frac{\bar{X_1}}{\bar{X_2}}$$

using logarithms to base $e$.

### Which continuous measure to use?
Hedge's _g_ and response ratios are the two most widely used metrics used in ecology. Both can be implemented in `metafor` for ease of calculation. There is some recent evidence that response ratios are more robust where there are big differences in the variance between studies; see Becky Spake's [recent paper in Ecology Letters](https://doi.org/10.1111/ele.13641) that she described in her seminar earlier in November.

## Manual meta-analysis of continuous data
### Overall weights W
Remember that in a meta-analysis you are basically calculating a weighted mean of the effect sizes. In this example we will use **Hedge's _g_** (bias-corrected standardised mean differences) for the individual studies from a systematic review. 

Now that we have calculated the bias-corrected variance $V_g$ we can readily calculate the weight _W_ for an individual study:

$$W = \frac{1}{V_{g}}$$

Again, this is very simple to calculate for all the studies simultaneously using R's vectorised arithmetic:

```{r weights_all_studies-setup}
J <- 1 - (3 / (4 * (continuous_dat$T_n + continuous_dat$C_n - 2) -1))
g <- J * d
Vd <- (continuous_dat$T_n+continuous_dat$C_n)/(continuous_dat$T_n*continuous_dat$C_n) +
   d^2/(2*(continuous_dat$T_n+continuous_dat$C_n))
Vg <- J^2 * Vd

```

```{r weights_all_studies, exercise=TRUE}
W <- 1 / Vg
cbind(continuous_dat$T_n, Vd, W)

```

Notice how the weights are related to the sample size and the variance, so for example study 4, which has 200 replicates per treatment, has a weight much higher than the others. The lowest weight is, however, given to study 4, within only 40 replicates, and the highest variance.

To finish, we calculate the weighted mean, _M_, based on the sum of weights multipled by the standardised effect sizes, divided by the sum of the weights:

$$M = \frac{\sum_{i=1}^{k}W_ig_i}{\sum_{i=1}^{k}W_i}$$
where:

* $W_i$ are the weights for study number $1$ to study number $k$
* $g_i$ are the Hedge's _g_ for each study
* $\sum$ Greek letter (sigma) for sum

This equation may look threatening at first, but it simply indicates that for each study (i = 1 to k, where k is 6 for this example) we carry out multiplications and additions.  The $\sum_{}$ sign indicates summation.  Again, the whole set of calculations can easily be done in R via vectorised arithmetic:

```{r calc_M-setup}
S_within <- sqrt(((continuous_dat$T_n-1)*continuous_dat$T_SD^2 +
                   (continuous_dat$C_n-1)*continuous_dat$C_SD^2) / 
                   (continuous_dat$T_n + continuous_dat$C_n -2))
d       <- (continuous_dat$T_mean - continuous_dat$C_mean) / S_within

J <- 1 - (3 / (4 * (continuous_dat$T_n + continuous_dat$C_n - 2) -1))
g <- J * d
Vd <- (continuous_dat$T_n+continuous_dat$C_n)/(continuous_dat$T_n*continuous_dat$C_n) +
   d^2/(2*(continuous_dat$T_n+continuous_dat$C_n))
Vg <- J^2 * Vd
W <- 1 / Vg
```

```{r calc_M, exercise=TRUE}
M <- sum(W * g) / sum(W)
M

```

This use of Hedge's _g_ gives an **overall standardised mean difference**, weighted for size of the different studies, with a bias-correction for studies with small numbers of replicates.

## Effect sizes for binary data
Often studies will report binary data, such as ‘dead’ or ‘alive’, ‘present’ or ‘absent’ etc. The three main ones for binary data are:

* Risk ratio
* Odds ratio
* Risk difference

The `metafor` package can handle all three type of effect size. The data will often be presented in the form of a 2 x 2 table in a single published study, although you would typically need to reformat the data to one row per study for a meta-analysis. Taking the example from page 33 of Borenstein:

| Intervention | Events | Non-events | N |
|:------------:|:----:|:-----:|:--:|
| Treated | A | B | $n_1$ |
| Control | C | D | $n_2$ |

then imagine you had a sample size of 100 for each of the two interventions:

| Intervention | Dead | Alive | N |
|:------------:|:----:|:-----:|:--:|
| Treated | 5 | 95 | 100 |
| Control | 10 | 90 | 100 |

Let's now look at these data for the three possible effect sizes

### Risk ratios
We have already looked at an example of **risk ratios** [in this website](https://naturalandenvironmentalscience.shinyapps.io/Effectsizes/#section-interactive-forest-plot-of-meta-analysis) and in ecological and conservation surveys if you are particularly interested in the risk of extinction for example, then these might be an appropriate measure.

In our Treated group above the risk of death is 5/100 = 0.05, whilst for the Control it is 10/100 = 0.10. Hence the risk ratio 0.05/0.10 = 0.5  Risk ratios are fairly intuitive, with values of less than 1.0 indicating the risk is lower in the control, a value of 1.0 indicates no difference, and value of greater than 1.0 indicates higher risk in the control. As explained before, to keep everything symmetrical the (natural) log is usually taken. **Note** In R the function for natural logarithms is `log()`.

$$RiskRatio=\frac{A/n_1}{C/n_2}$$

and

$$LogRiskRatio=ln(RiskRatio)$$

The variance is approximately:

$$V_{LogRiskRatio}=\frac{1}{A}-\frac{1}{n_1}+\frac{1}{C}-\frac{1}{n_2}$$

with an approximation of the standard error being

$$SE_{LogRiskRatio}=\sqrt{V_{LogRiskRatio}}$$

In this code box we can now easily complete these calculations:

```{r RR, exercise=TRUE}
logRR <- log((5/100)/(10/100))
logRR
VRR   <- 1/5 - 1/100 + 1/10 - 1/100
VRR
VSE   <- sqrt(VRR)
VSE
```


### Odds ratios
The odds of death in our Treated group is 5/95=0.0526, and the odds of death in the Control is 10/90=0.1111  The odds ratio is therefore 0.0526/0.1111 = 0.4737  Again the log odds ratio is often used. Formally, the equation is:

$$OddsRatio=\frac{AD}{BC}$$

Like most people I don't find this as intuitive as Risk Ratios, but odds ratios have certain statistical advantages, so you should be familiar with them. For completeness:

$$LogOddsRatio=ln(OddsRatio)$$

with the variance given by:

$$V_{LogOddsRatio}=\frac{1}{A}+\frac{1}{B}+\frac{1}{C}+\frac{1}{D}$$

and standard error:

$$SE_{LogOddsRatio}=\sqrt{V_{LogOddsRatio}}$$

Again, these calculations are simple to complete in R:

```{r OR, exercise = TRUE}
LogOR  <- log((5 * 90)/(95 * 10))
LogOR
VOR    <- 1/5 + 1/95 + 1/10 + 1/90
VOR
SEOR   <- sqrt(VOR)
SEOR
```


For both risk ratios and odds ratios it doesn't matter whether you pick 'Treated' or 'Control' as your numerator, as long as you are consistent across all your studies.

### Risk differences
This is the difference between two risks, and we work in the original units without log-transformation:

$$RiskDifference=(\frac{A}{n_1})-(\frac{C}{n_2})$$

the variance is:

$$V_{RiskDifference}=\frac{AB}{n_1^3}+\frac{CD}{n_2^3}$$

and the standard error is:

$$SE_{RiskDifference}=\sqrt{V_{RiskDifference}}$$

These calculations are shown in the code block below:

```{r RD, exercise = TRUE}
RD <- (5/100) - (10/100)
RD
VRD <- 5*95/100^3 + 10*90/100^3 # Note use of ^3 to raise to cube
VRD
SERD <- sqrt(VRD)
SERD
```

### Which binary metric to choose?
Risk ratios and odds ratios are most commonly used, as they are **relative** measures and not affected by the absolute size of the baseline against which you are comparing. Risk ratios are probably easier to interpret than odds ratios; you will learn how to compute both in `metafor`.

## Effect sizes for correlations
Most ecologists are very familiar with correlation, as it is a relatively simple statistic, widely reported. The "population" correlation is usually represented by the Greek letter $\rho$ (Greek letter _rho_) whilst the "sample" correlation, that is what is actually reported in any study, is indicated via _r_.   However, in meta-analysis the correlation coefficients _r_ are usually converted to Fisher's _z_ scale (the terminology is a little confusing, as this is not the same as the _z_ values used in some statistics tests). The values are then back-converted to the original _r_ values for final presentation of the meta-analysis results. The process is shown schematically below:

![](www/correl_fisher_z_fig_6_1.png)

and Fisher's _z_ scale values can be calculated as:

$$z=0.5 \times \ln\bigg(\frac{1+r}{1-r}\bigg)$$
and the back-transformation to the original correlation units is:

$$r=\frac{e^{2z}-1}{e^{2z}+1}$$
Again we will use one of the example datasets from Borenstein, so that if you want to undertake the entire analysis by hand to gain greater insights you a free to do so.  First, import the data:

```{r correlation, exercise=TRUE}
corr_dat

```

There are six studies; to manually calculate Fisher's _z_ transformed value for the first study by Fonda, we simply need to calculate:

$$z_1 = 0.5 \times \ln\bigg(\frac{1+0.50}{1-0.50}\bigg)$$

which can be calculated in R; remember that in R the function `log()` is for natural logs, whereas the function `log10()` is for logs to base-10:

```{r z value}
z_Fonda <- 0.5 * log((1+corr_dat$Correlation[1])/(1-corr_dat$Correlation[1]))
z_Fonda

```

This is quite cumbersome for one study, but fortunately the calculations can be done automatically within `metafor`.

